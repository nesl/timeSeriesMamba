Using Framework:  BackboneModel
configs.llm_model:  Mamba2
defined config MambaConfig(d_model=256, d_intermediate=0, n_layer=32, vocab_size=50277, ssm_cfg={'layer': 'Mamba2'}, attn_layer_idx=[], attn_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)
LLM model used is:  Mamba2
[2024-07-18 14:28:24,321] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [39m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [39m async_io: please install the libaio-dev package with apt
[93m [WARNING] [39m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [39m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [39m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
Total number of parameters: 79893712












93it [01:57,  4.10it/s]
	iters: 100, epoch: 1 | loss: 0.5339247












194it [02:21,  4.16it/s]
	iters: 200, epoch: 1 | loss: 0.7922796












293it [02:45,  4.20it/s]
	iters: 300, epoch: 1 | loss: 0.3206528












393it [03:09,  4.08it/s]
	iters: 400, epoch: 1 | loss: 0.6784322


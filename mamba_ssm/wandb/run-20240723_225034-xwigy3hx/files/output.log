Using Framework:  BackboneModel
args.device:  cuda:0
configs.llm_model:  Mamba
defined config MambaConfig(d_model=256, d_intermediate=0, n_layer=32, vocab_size=50277, ssm_cfg={'layer': 'Mamba'}, attn_layer_idx=[], attn_cfg={}, rms_norm=True, residual_in_fp32=True, fused_add_norm=True, pad_vocab_size_multiple=8, tie_embeddings=True)
Traceback (most recent call last):
  File "/home/nesl/oliver/timeSeriesMamba/mamba_ssm/train.py", line 164, in <module>
    model = BackboneModel.Model(args).float()
            ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nesl/oliver/timeSeriesMamba/mamba_ssm/models/BackboneModel.py", line 63, in __init__
    self.llm_model = MambaTimeHeadModel.from_init(configs, device=self.device, dtype=self.dtype)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nesl/oliver/timeSeriesMamba/mamba_ssm/models/mixer_seq_simple.py", line 425, in from_init
    model = cls(config, device=device, dtype=dtype, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nesl/oliver/timeSeriesMamba/mamba_ssm/models/mixer_seq_simple.py", line 349, in __init__
    self.backbone = MixerModel(
                    ^^^^^^^^^^^
  File "/home/nesl/oliver/timeSeriesMamba/mamba_ssm/models/mixer_seq_simple.py", line 140, in __init__
    self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nesl/anaconda3/envs/TimeMamba/lib/python3.11/site-packages/torch/nn/modules/sparse.py", line 143, in __init__
    self.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt